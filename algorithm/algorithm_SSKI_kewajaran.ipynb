{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1feeda39-1dca-451c-b43b-1a4fd437bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import datetime\n",
    "import calendar\n",
    "import numpy as np # type: ignore\n",
    "import re\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "290878ad-23c0-45b5-b7ef-c4c5ac36ddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "super_sheet = pd.ExcelFile(r\"/Users/ferroyudisthira/Desktop/DSTA_DQAD/V&H_Check/Sumber_Data_Lama/SSKI/SSKI EKSTERNAL_25 Okt 2024.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a953f7a-d237-466e-be07-35d8c975ce34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO</th>\n",
       "      <th>Komponen</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>...</th>\n",
       "      <th>2022-Q2</th>\n",
       "      <th>2022-Q3</th>\n",
       "      <th>2022-Q4</th>\n",
       "      <th>2023</th>\n",
       "      <th>2023-Q1</th>\n",
       "      <th>2023-Q2</th>\n",
       "      <th>2023-Q3</th>\n",
       "      <th>2023-Q4</th>\n",
       "      <th>2024-Q1</th>\n",
       "      <th>2024-Q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Core FSIs for Deposit takers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Regulatory capital to risk-weighted assets</td>\n",
       "      <td>2.018410e+01</td>\n",
       "      <td>2.125834e+01</td>\n",
       "      <td>1.952652e+01</td>\n",
       "      <td>1.663261e+01</td>\n",
       "      <td>1.715633e+01</td>\n",
       "      <td>1.583715e+01</td>\n",
       "      <td>1.524985e+01</td>\n",
       "      <td>1.616445e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.310304e+01</td>\n",
       "      <td>2.352603e+01</td>\n",
       "      <td>2.413405e+01</td>\n",
       "      <td>2.584111e+01</td>\n",
       "      <td>2.321433e+01</td>\n",
       "      <td>2.525690e+01</td>\n",
       "      <td>2.547301e+01</td>\n",
       "      <td>2.584111e+01</td>\n",
       "      <td>24.2906</td>\n",
       "      <td>24.437104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Total regulatory capital</td>\n",
       "      <td>1.499328e+08</td>\n",
       "      <td>1.680040e+08</td>\n",
       "      <td>1.955812e+08</td>\n",
       "      <td>2.124505e+08</td>\n",
       "      <td>2.404399e+08</td>\n",
       "      <td>2.830622e+08</td>\n",
       "      <td>3.577378e+08</td>\n",
       "      <td>4.380935e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.426490e+09</td>\n",
       "      <td>1.474245e+09</td>\n",
       "      <td>1.545912e+09</td>\n",
       "      <td>1.663298e+09</td>\n",
       "      <td>1.507131e+09</td>\n",
       "      <td>1.553433e+09</td>\n",
       "      <td>1.598943e+09</td>\n",
       "      <td>1.663298e+09</td>\n",
       "      <td>1604557738.256399</td>\n",
       "      <td>1651639225.5341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Risk-weighted assets</td>\n",
       "      <td>7.428265e+08</td>\n",
       "      <td>7.902968e+08</td>\n",
       "      <td>1.001618e+09</td>\n",
       "      <td>1.277313e+09</td>\n",
       "      <td>1.401464e+09</td>\n",
       "      <td>1.787330e+09</td>\n",
       "      <td>2.345845e+09</td>\n",
       "      <td>2.710228e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>6.174467e+09</td>\n",
       "      <td>6.266440e+09</td>\n",
       "      <td>6.405521e+09</td>\n",
       "      <td>6.436634e+09</td>\n",
       "      <td>6.492244e+09</td>\n",
       "      <td>6.150531e+09</td>\n",
       "      <td>6.277008e+09</td>\n",
       "      <td>6.436634e+09</td>\n",
       "      <td>6605673526.403298</td>\n",
       "      <td>6758735617.138998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tier 1 capital to risk-weighted assets</td>\n",
       "      <td>1.544958e+01</td>\n",
       "      <td>1.644884e+01</td>\n",
       "      <td>1.558629e+01</td>\n",
       "      <td>1.373710e+01</td>\n",
       "      <td>1.432014e+01</td>\n",
       "      <td>1.400932e+01</td>\n",
       "      <td>1.350456e+01</td>\n",
       "      <td>1.422485e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.139775e+01</td>\n",
       "      <td>2.184782e+01</td>\n",
       "      <td>2.255920e+01</td>\n",
       "      <td>2.418378e+01</td>\n",
       "      <td>2.168986e+01</td>\n",
       "      <td>2.365053e+01</td>\n",
       "      <td>2.388114e+01</td>\n",
       "      <td>2.418378e+01</td>\n",
       "      <td>22.744696</td>\n",
       "      <td>22.931249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>212</td>\n",
       "      <td>Total gross loans</td>\n",
       "      <td>8.221774e+08</td>\n",
       "      <td>9.173799e+08</td>\n",
       "      <td>1.107123e+09</td>\n",
       "      <td>1.473261e+09</td>\n",
       "      <td>1.662246e+09</td>\n",
       "      <td>1.961254e+09</td>\n",
       "      <td>2.381914e+09</td>\n",
       "      <td>3.020937e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>6.776339e+09</td>\n",
       "      <td>7.051318e+09</td>\n",
       "      <td>7.295716e+09</td>\n",
       "      <td>7.925380e+09</td>\n",
       "      <td>7.303767e+09</td>\n",
       "      <td>7.543825e+09</td>\n",
       "      <td>7.733890e+09</td>\n",
       "      <td>7.925380e+09</td>\n",
       "      <td>8000771521.443933</td>\n",
       "      <td>8155013645.16169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>213</td>\n",
       "      <td>Commercial real estate loans to total gross loans</td>\n",
       "      <td>8.656033e-01</td>\n",
       "      <td>1.208650e+00</td>\n",
       "      <td>1.425522e+00</td>\n",
       "      <td>1.525244e+00</td>\n",
       "      <td>1.318092e+00</td>\n",
       "      <td>5.025799e+00</td>\n",
       "      <td>1.483590e+01</td>\n",
       "      <td>1.018863e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.544998e+01</td>\n",
       "      <td>1.517310e+01</td>\n",
       "      <td>1.502278e+01</td>\n",
       "      <td>1.469755e+01</td>\n",
       "      <td>1.514551e+01</td>\n",
       "      <td>1.491263e+01</td>\n",
       "      <td>1.476139e+01</td>\n",
       "      <td>1.469755e+01</td>\n",
       "      <td>14.560103</td>\n",
       "      <td>14.999295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>214</td>\n",
       "      <td>Commercial real estate loans</td>\n",
       "      <td>7.116795e+06</td>\n",
       "      <td>1.108791e+07</td>\n",
       "      <td>1.578228e+07</td>\n",
       "      <td>2.247082e+07</td>\n",
       "      <td>2.190993e+07</td>\n",
       "      <td>9.856868e+07</td>\n",
       "      <td>3.533785e+08</td>\n",
       "      <td>3.077921e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>1.046943e+09</td>\n",
       "      <td>1.069903e+09</td>\n",
       "      <td>1.096020e+09</td>\n",
       "      <td>1.164837e+09</td>\n",
       "      <td>1.106193e+09</td>\n",
       "      <td>1.124982e+09</td>\n",
       "      <td>1.141630e+09</td>\n",
       "      <td>1.164837e+09</td>\n",
       "      <td>1164920582.555511</td>\n",
       "      <td>1223194550.859583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>215</td>\n",
       "      <td>Total gross loans</td>\n",
       "      <td>8.221774e+08</td>\n",
       "      <td>9.173799e+08</td>\n",
       "      <td>1.107123e+09</td>\n",
       "      <td>1.473261e+09</td>\n",
       "      <td>1.662246e+09</td>\n",
       "      <td>1.961254e+09</td>\n",
       "      <td>2.381914e+09</td>\n",
       "      <td>3.020937e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>6.776339e+09</td>\n",
       "      <td>7.051318e+09</td>\n",
       "      <td>7.295716e+09</td>\n",
       "      <td>7.925380e+09</td>\n",
       "      <td>7.303767e+09</td>\n",
       "      <td>7.543825e+09</td>\n",
       "      <td>7.733890e+09</td>\n",
       "      <td>7.925380e+09</td>\n",
       "      <td>8000771521.443933</td>\n",
       "      <td>8155013645.16169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      NO                                           Komponen          2005  \\\n",
       "0      1                       Core FSIs for Deposit takers           NaN   \n",
       "1      2         Regulatory capital to risk-weighted assets  2.018410e+01   \n",
       "2      3                           Total regulatory capital  1.499328e+08   \n",
       "3      4                               Risk-weighted assets  7.428265e+08   \n",
       "4      5             Tier 1 capital to risk-weighted assets  1.544958e+01   \n",
       "..   ...                                                ...           ...   \n",
       "211  212                                  Total gross loans  8.221774e+08   \n",
       "212  213  Commercial real estate loans to total gross loans  8.656033e-01   \n",
       "213  214                       Commercial real estate loans  7.116795e+06   \n",
       "214  215                                  Total gross loans  8.221774e+08   \n",
       "215  NaN                                                NaN           NaN   \n",
       "\n",
       "             2006          2007          2008          2009          2010  \\\n",
       "0             NaN           NaN           NaN           NaN           NaN   \n",
       "1    2.125834e+01  1.952652e+01  1.663261e+01  1.715633e+01  1.583715e+01   \n",
       "2    1.680040e+08  1.955812e+08  2.124505e+08  2.404399e+08  2.830622e+08   \n",
       "3    7.902968e+08  1.001618e+09  1.277313e+09  1.401464e+09  1.787330e+09   \n",
       "4    1.644884e+01  1.558629e+01  1.373710e+01  1.432014e+01  1.400932e+01   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "211  9.173799e+08  1.107123e+09  1.473261e+09  1.662246e+09  1.961254e+09   \n",
       "212  1.208650e+00  1.425522e+00  1.525244e+00  1.318092e+00  5.025799e+00   \n",
       "213  1.108791e+07  1.578228e+07  2.247082e+07  2.190993e+07  9.856868e+07   \n",
       "214  9.173799e+08  1.107123e+09  1.473261e+09  1.662246e+09  1.961254e+09   \n",
       "215           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "             2011          2012  ...       2022-Q2       2022-Q3  \\\n",
       "0             NaN           NaN  ...           NaN           NaN   \n",
       "1    1.524985e+01  1.616445e+01  ...  2.310304e+01  2.352603e+01   \n",
       "2    3.577378e+08  4.380935e+08  ...  1.426490e+09  1.474245e+09   \n",
       "3    2.345845e+09  2.710228e+09  ...  6.174467e+09  6.266440e+09   \n",
       "4    1.350456e+01  1.422485e+01  ...  2.139775e+01  2.184782e+01   \n",
       "..            ...           ...  ...           ...           ...   \n",
       "211  2.381914e+09  3.020937e+09  ...  6.776339e+09  7.051318e+09   \n",
       "212  1.483590e+01  1.018863e+01  ...  1.544998e+01  1.517310e+01   \n",
       "213  3.533785e+08  3.077921e+08  ...  1.046943e+09  1.069903e+09   \n",
       "214  2.381914e+09  3.020937e+09  ...  6.776339e+09  7.051318e+09   \n",
       "215           NaN           NaN  ...           NaN           NaN   \n",
       "\n",
       "          2022-Q4          2023       2023-Q1       2023-Q2       2023-Q3  \\\n",
       "0             NaN           NaN           NaN           NaN           NaN   \n",
       "1    2.413405e+01  2.584111e+01  2.321433e+01  2.525690e+01  2.547301e+01   \n",
       "2    1.545912e+09  1.663298e+09  1.507131e+09  1.553433e+09  1.598943e+09   \n",
       "3    6.405521e+09  6.436634e+09  6.492244e+09  6.150531e+09  6.277008e+09   \n",
       "4    2.255920e+01  2.418378e+01  2.168986e+01  2.365053e+01  2.388114e+01   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "211  7.295716e+09  7.925380e+09  7.303767e+09  7.543825e+09  7.733890e+09   \n",
       "212  1.502278e+01  1.469755e+01  1.514551e+01  1.491263e+01  1.476139e+01   \n",
       "213  1.096020e+09  1.164837e+09  1.106193e+09  1.124982e+09  1.141630e+09   \n",
       "214  7.295716e+09  7.925380e+09  7.303767e+09  7.543825e+09  7.733890e+09   \n",
       "215           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "          2023-Q4            2024-Q1            2024-Q2  \n",
       "0             NaN                  .                  .  \n",
       "1    2.584111e+01            24.2906          24.437104  \n",
       "2    1.663298e+09  1604557738.256399    1651639225.5341  \n",
       "3    6.436634e+09  6605673526.403298  6758735617.138998  \n",
       "4    2.418378e+01          22.744696          22.931249  \n",
       "..            ...                ...                ...  \n",
       "211  7.925380e+09  8000771521.443933   8155013645.16169  \n",
       "212  1.469755e+01          14.560103          14.999295  \n",
       "213  1.164837e+09  1164920582.555511  1223194550.859583  \n",
       "214  7.925380e+09  8000771521.443933   8155013645.16169  \n",
       "215           NaN                NaN                NaN  \n",
       "\n",
       "[216 rows x 71 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_dataframe(sheet_name, header_count):\n",
    "    if sheet_name in ['11','16']:\n",
    "        input_df = super_sheet.parse(sheet_name, header=[3, 2 + header_count])\n",
    "    else:\n",
    "        input_df = super_sheet.parse(sheet_name, header=[4, 3 + header_count])\n",
    "\n",
    "    # Clean up and modify the column headers\n",
    "    input_df.columns = input_df.columns.map(lambda x: str(x[0]).upper() if 'Unnamed' in str(x) else x)\n",
    "\n",
    "    # Strip spaces in tuple columns\n",
    "    input_df.columns = input_df.columns.map(lambda x: (x[0], x[1].strip()) if isinstance(x, tuple) and isinstance(x[1], str) else x)\n",
    "\n",
    "    # Combine the multi-level column headers into a single string\n",
    "    input_df.columns = input_df.columns.map(lambda x: f'{x[0]}-{x[1].capitalize()}' if isinstance(x, tuple) else x)\n",
    "\n",
    "    first_column = input_df.iloc[:, 0]\n",
    "    second_column = input_df.iloc[:, 1]\n",
    "\n",
    "    # Filter based on the year (assuming year_to_check is a variable in scope)\n",
    "    input_df = input_df.filter(regex=str(20))\n",
    "\n",
    "    # Insert the 'NO' and 'Komponen' columns\n",
    "    input_df.insert(0, 'NO', first_column)\n",
    "    input_df.insert(1, 'Komponen', second_column)\n",
    "\n",
    "    # Remove rows after the 'Keterangan' row (if it exists)\n",
    "    index_keterangan = input_df[input_df.apply(lambda row: row.astype(str).str.contains('keterangan', case=False).any(), axis=1)].index\n",
    "\n",
    "    if not index_keterangan.empty:\n",
    "        first_keterangan_index = index_keterangan[0]\n",
    "        input_df = input_df.iloc[:first_keterangan_index]\n",
    "\n",
    "    return input_df\n",
    "\n",
    "df = prepare_dataframe(\"4\", 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bea371ab-f105-4e66-9143-72be1dced613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe5(sheet_name):\n",
    "    if sheet_name == '5a':\n",
    "        header_array = [3, 4, 5]\n",
    "    else:\n",
    "        header_array = [2, 3, 4, 5]\n",
    "\n",
    "    input_df = super_sheet.parse(sheet_name, header=header_array)\n",
    "\n",
    "    input_df.columns = input_df.columns.map(lambda x: str(x[0]).upper() if 'Unnamed' in str(x) else x)\n",
    "\n",
    "    input_df.columns = input_df.columns.map(\n",
    "        lambda x: tuple(\n",
    "            str(x[i]).upper() if i == 0 and 'Unnamed' in str(x[1]) else x[i].strip() if isinstance(x[i], str) else x[i]\n",
    "            for i in range(len(x))\n",
    "        ) if isinstance(x, tuple) else x\n",
    "    )\n",
    "\n",
    "    # Remove row with \"keterangan\" and rows following\n",
    "    index_keterangan = input_df[input_df.apply(lambda row: row.astype(str).str.contains('keterangan', case=False).any(), axis=1)].index\n",
    "\n",
    "    if not index_keterangan.empty:\n",
    "        first_keterangan_index = index_keterangan[0]\n",
    "        input_df = input_df.iloc[:first_keterangan_index]\n",
    "    \n",
    "    # Gabungkan elemen kolom multi-level menjadi satu string secara fleksibel berdasarkan panjang tuple\n",
    "    input_df.columns = input_df.columns.map(lambda x: '-'.join([str(i).capitalize() if isinstance(i, str) else str(i) for i in x]) if isinstance(x, tuple) else x)\n",
    "\n",
    "    first_column = input_df.iloc[:, 0]\n",
    "    second_column = input_df.iloc[:, 1]\n",
    "\n",
    "    # Filter columns where the name contains a year (assuming year_to_check is a variable in scope)\n",
    "    input_df = input_df.filter(regex=str(20))\n",
    "\n",
    "    # Insert the 'NO' and 'Komponen' columns\n",
    "    input_df.insert(0, 'NO', first_column)\n",
    "    input_df.insert(1, 'Komponen', second_column)\n",
    "\n",
    "    # Remove rows after the 'Keterangan' row (if it exists)\n",
    "    index_keterangan = input_df[input_df.apply(lambda row: row.astype(str).str.contains('keterangan', case=False).any(), axis=1)].index\n",
    "\n",
    "    if not index_keterangan.empty:\n",
    "        first_keterangan_index = index_keterangan[0]\n",
    "        input_df = input_df.iloc[:first_keterangan_index]\n",
    "\n",
    "    # Drop columns that contain only a year as their name (e.g., '2020', '2021')\n",
    "    input_df = input_df.loc[:, ~input_df.columns.str.match(r'^\\d{4}$')]\n",
    "\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95f0cfc7-cb8a-40ee-85ea-80e1aac9d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split5a(df):\n",
    "    claims_df = df.filter(like='Claims')\n",
    "    liabilities_df = df.filter(like='Liabilities')\n",
    "    common_columns = df.filter(regex='NO|Komponen')\n",
    "    # Menggabungkan kolom Nomor dan Komponen ke DataFrame Claims dan Liabilities\n",
    "    claims_df = pd.concat([common_columns, claims_df], axis=1)\n",
    "    liabilities_df = pd.concat([common_columns, liabilities_df], axis=1)\n",
    "    return claims_df, liabilities_df\n",
    "\n",
    "def split5b(df):\n",
    "    # Extract common columns (No and Komponen)\n",
    "    common_columns = df.filter(regex='NO|Komponen')\n",
    "    \n",
    "    # Filter for Claims and Liabilities by categories using df.filter(like=...)\n",
    "    claims_df_total = pd.concat([common_columns, df.filter(like='Claims-Total')], axis=1)\n",
    "    liabilities_df_total = pd.concat([common_columns, df.filter(like='Liabilities-Total')], axis=1)\n",
    "    claims_df_ld = pd.concat([common_columns, df.filter(like='Claims-Loan & deposits')], axis=1)\n",
    "    liabilities_df_ld = pd.concat([common_columns, df.filter(like='Liabilities-Loan & deposits')], axis=1)\n",
    "    claims_df_ds = pd.concat([common_columns, df.filter(like='Claims-Debt securities')], axis=1)\n",
    "    liabilities_df_ds = pd.concat([common_columns, df.filter(like='Liabilities-Debt securities')], axis=1)\n",
    "    claims_df_oi = pd.concat([common_columns, df.filter(like='Claims-Other instruments')], axis=1)\n",
    "    liabilities_df_oi = pd.concat([common_columns, df.filter(like='Liabilities-Other instruments')], axis=1)\n",
    "    \n",
    "    # Return all the dataframes\n",
    "    return claims_df_total, liabilities_df_total, claims_df_ld, liabilities_df_ld, claims_df_ds, liabilities_df_ds, claims_df_oi, liabilities_df_oi\n",
    "\n",
    "def split5d(df):\n",
    "    # Extract common columns (No and Komponen)\n",
    "    common_columns = df.filter(regex='NO|Komponen')\n",
    "    \n",
    "    # Filter for Claims and Liabilities by categories using df.filter(like=...) and concatenate with common columns\n",
    "    claims_df_total = pd.concat([common_columns, df.filter(like='Claims-Total')], axis=1)\n",
    "    liabilities_df_total = pd.concat([common_columns, df.filter(like='Liabilities-Total')], axis=1)\n",
    "    claims_df_b = pd.concat([common_columns, df.filter(like='Claims-Banks')], axis=1)\n",
    "    liabilities_df_b = pd.concat([common_columns, df.filter(like='Liabilities-Banks')], axis=1)\n",
    "    claims_df_nb = pd.concat([common_columns, df.filter(like='Claims-Nonbanks')], axis=1)\n",
    "    liabilities_df_nb = pd.concat([common_columns, df.filter(like='Liabilities-Nonbanks')], axis=1)\n",
    "    \n",
    "    # Return all the dataframes\n",
    "    return claims_df_total, liabilities_df_total, claims_df_b, liabilities_df_b, claims_df_nb, liabilities_df_nb\n",
    "\n",
    "def split5d1(df):\n",
    "    # Extract common columns (No and Komponen)\n",
    "    common_columns = df.filter(regex='NO|Komponen')\n",
    "    \n",
    "    # Filter for Claims and Liabilities by categories using df.filter(like=...) and concatenate with common columns\n",
    "    claims_df_total = pd.concat([common_columns, df.filter(like='Claims-Total')], axis=1)\n",
    "    liabilities_df_total = pd.concat([common_columns, df.filter(like='Liabilities-Total')], axis=1)\n",
    "    claims_df_r = pd.concat([common_columns, df.filter(like='Claims-Related offices')], axis=1)\n",
    "    liabilities_df_r = pd.concat([common_columns, df.filter(like='Liabilities-Related offices')], axis=1)\n",
    "    \n",
    "    # Return all the dataframes\n",
    "    return claims_df_total, liabilities_df_total, claims_df_r, liabilities_df_r\n",
    "\n",
    "def split5d2(df):\n",
    "    # Extract common columns (No and Komponen)\n",
    "    common_columns = df.filter(regex='NO|Komponen')\n",
    "    \n",
    "    # Filter for Claims and Liabilities by categories using df.filter(like=...) and concatenate with common columns\n",
    "    claims_df_total = pd.concat([common_columns, df.filter(like='Claims-Total')], axis=1)\n",
    "    liabilities_df_total = pd.concat([common_columns, df.filter(like='Liabilities-Total')], axis=1)\n",
    "    claims_df_nb = pd.concat([common_columns, df.filter(like='Claims-Non-bank financial ins.')], axis=1)\n",
    "    liabilities_df_nb = pd.concat([common_columns, df.filter(like='Liabilities-Non-bank financial ins.')], axis=1)\n",
    "    claims_df_nf = pd.concat([common_columns, df.filter(like='Claims-Non-financial corporations')], axis=1)\n",
    "    liabilities_df_nf = pd.concat([common_columns, df.filter(like='Liabilities-Non-financial corporations')], axis=1)\n",
    "    \n",
    "    # Return all the dataframes\n",
    "    return claims_df_total, liabilities_df_total, claims_df_nb, liabilities_df_nb, claims_df_nf, liabilities_df_nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11ca5b9b-7123-46f4-a025-fbdb0f867013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all(name, input_df):\n",
    "\n",
    "    # Step 2: Filter the DataFrame based on the selected columns\n",
    "    filtered_df = input_df\n",
    "\n",
    "    # Step 3: Replace '-' with NaN and drop rows with NaN\n",
    "    filtered_df.replace('-', np.nan, inplace=True)\n",
    "\n",
    "    # Step 4: Transpose the table, so NO and Komponen become the headers\n",
    "    transposed_df = filtered_df.set_index(['NO', 'Komponen']).T\n",
    "    \n",
    "    return transposed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ae0ad1d-4ba9-44c8-99f6-24722a83b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Month mapping dictionary\n",
    "month_mapping = {\n",
    "    'Jan': 'Jan',\n",
    "    'Feb': 'Feb',\n",
    "    'Mar': 'Mar',\n",
    "    'Apr': 'Apr',\n",
    "    'Mei': 'May',\n",
    "    'Jun': 'Jun',\n",
    "    'Jul': 'Jul',\n",
    "    'Agt': 'Aug',\n",
    "    'Sept': 'Sep',\n",
    "    'Okt': 'Oct',\n",
    "    'Nov': 'Nov',\n",
    "    'Des': 'Dec',\n",
    "    'Q1': 'Mar',\n",
    "    'Q2': 'Jun',\n",
    "    'Q3': 'Sep',\n",
    "    'Q4': 'Dec'\n",
    "}\n",
    "\n",
    "# Function to reformat Periode\n",
    "def reformat_periode(periode):\n",
    "    tokens = periode.split('-')\n",
    "    \n",
    "    if len(tokens) > 1:\n",
    "        year = tokens[0]\n",
    "        month = tokens[1]\n",
    "        # Clean the month in case there are additional characters like 'Dec*' or 'OctX'\n",
    "        month = re.sub(r'[^A-Za-z0-9]', '', month)\n",
    "        \n",
    "        # Map the cleaned month to the dictionary\n",
    "        return f\"{year}-{month_mapping.get(month, month)}\"\n",
    "    \n",
    "    elif len(tokens) == 1:\n",
    "        # Single-token period handling\n",
    "        cleaned_token = re.sub(r'[^A-Za-z0-9]', '', tokens[0])  # Clean the single token\n",
    "        return month_mapping.get(cleaned_token, cleaned_token)\n",
    "    \n",
    "    return periode  # Handle unexpected cases gracefully\n",
    "\n",
    "def detect_outliers_iqr(name,df):\n",
    "    # Pastikan DataFrame tidak kosong sebelum melanjutkan\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty.\")\n",
    "        return pd.DataFrame(columns=['Komponen','Detail', 'Tahun', 'Nilai', 'Batas Bawah', 'Batas Atas', 'Outlier', 'Gap', 'last_period'])\n",
    "    \n",
    "    if name == \"5.d.2\":\n",
    "        name = \"5d2\"\n",
    "    \n",
    "    # Replace string values (including '-') with NaN\n",
    "    df = df.replace('-', np.nan).map(lambda x: np.nan if isinstance(x, str) else x)\n",
    "    \n",
    "    # Get the last period for each component (assuming index represents period/year)\n",
    "    last_period = df.index[-1] if len(df.index) > 0 else None\n",
    "\n",
    "    outliers_data = []\n",
    "    for column in df.columns:\n",
    "        # Ensure all values in the column are numeric\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "        col_data = df[column].dropna()\n",
    "        \n",
    "        if col_data.empty:\n",
    "            continue  # Skip columns with no numeric data\n",
    "        \n",
    "        # Calculate IQR, lower bound, and upper bound\n",
    "        Q1 = col_data.quantile(0.25)\n",
    "        Q3 = col_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Process each value and identify if it's an outlier\n",
    "        for index, value in df[column].items():\n",
    "            is_outlier = value < lower_bound or value > upper_bound\n",
    "            \n",
    "            # Calculate gap only for outliers, leave empty for non-outliers\n",
    "            gap = None\n",
    "            if is_outlier:\n",
    "                gap = (value - upper_bound) if value > upper_bound else (lower_bound - value)\n",
    "            token = index.split(\"-\",2)\n",
    "            if len(token) == 3:\n",
    "                detail = token[2]\n",
    "            else:\n",
    "                detail = '-'\n",
    "\n",
    "\n",
    "            outliers_data.append({\n",
    "                'Tabel' : name,\n",
    "                'No_Komponen':int(column[0]),\n",
    "                'Komponen': column[1],\n",
    "                'Detail' : detail,\n",
    "                'Periode': index,\n",
    "                'Nilai': value,\n",
    "                'Batas Bawah': lower_bound,\n",
    "                'Batas Atas': upper_bound,\n",
    "                'Outlier': 'Ya' if is_outlier else 'Tidak',\n",
    "                'Gap': gap,\n",
    "                'last_period': last_period\n",
    "            })\n",
    "    \n",
    "    # Create the DataFrame with the collected outlier data\n",
    "    df_outliers = pd.DataFrame(outliers_data)\n",
    "\n",
    "    # # Apply the function to the Periode column\n",
    "    df_outliers['Periode'] = df_outliers['Periode'].apply(reformat_periode)\n",
    "    df_outliers['last_period'] = df_outliers['last_period'].apply(reformat_periode)\n",
    "\n",
    "    \n",
    "    return df_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7982d2d7-1071-4267-a96d-1bed1349d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"1\"\n",
    "# raw_df = prepare_dataframe(name, 2)\n",
    "# checked_df = prepare_all(name, raw_df)\n",
    "# checked_df = checked_df.iloc[1:]\n",
    "# outliers_df = detect_outliers_iqr(name,checked_df)\n",
    "# # Add a new column 'No.' that contains the index values\n",
    "# outliers_df.insert(0, 'No.', range(1, len(outliers_df) + 1))\n",
    "# # Display the updated DataFrame\n",
    "# display(outliers_df)\n",
    "\n",
    "# output_path = r\"/Users/ferroyudisthira/Desktop/DSTA_DQAD/V&H_Check/application/data/data_pencilan_sski.csv\"\n",
    "# outliers_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99205aa3-8a58-4213-80d1-0780a9d9b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SSKI 1...\n",
      "Processing SSKI 2...\n",
      "Processing SSKI 3...\n",
      "Processing SSKI 4...\n",
      "Processing SSKI 5a...\n",
      "Processing SSKI 5b...\n",
      "Processing SSKI 5c...\n",
      "Processing SSKI 5d...\n",
      "Processing SSKI 5d.1...\n",
      "Processing SSKI 5.d.2...\n",
      "Processing SSKI 6...\n",
      "Processing SSKI 7...\n",
      "Processing SSKI 8...\n",
      "Processing SSKI 9...\n",
      "Processing SSKI 10...\n",
      "Processing SSKI 11...\n",
      "Processing SSKI 11a...\n",
      "Processing SSKI 12...\n",
      "Processing SSKI 13...\n",
      "Processing SSKI 14...\n",
      "Processing SSKI 15...\n",
      "Processing SSKI 16...\n",
      "Processing SSKI 16a...\n",
      "Processing SSKI 17...\n",
      "Processing SSKI 18...\n",
      "Processing SSKI 19...\n",
      "Processing SSKI 20...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sheet_names = [\"1\",\"2\",\"3\",\"4\",\"5a\",\"5b\",\"5c\",\"5d\",\"5d.1\",\"5.d.2\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"11a\",\"12\",\"13\",\"14\",\"15\",\"16\",\"16a\",\"17\",\"18\",\"19\",\"20\"]  \n",
    "\n",
    "all_outliers = []\n",
    "\n",
    "# Loop to process all sheets\n",
    "for name in sheet_names:\n",
    "    print(f\"Processing SSKI {name}...\")\n",
    "\n",
    "    if name == \"5a\":\n",
    "        raw_df = prepare_dataframe5(name)\n",
    "        \n",
    "        claims_df, liabilities_df = split5a(raw_df)\n",
    "        \n",
    "        claims_df = prepare_all(name, claims_df)\n",
    "        liabilities_df = prepare_all(name, liabilities_df)\n",
    "        \n",
    "        claims_outliers = detect_outliers_iqr(name, claims_df)\n",
    "        all_outliers.append(claims_outliers)\n",
    "\n",
    "        liabilities_outliers = detect_outliers_iqr(name, liabilities_df)\n",
    "        all_outliers.append(liabilities_outliers)\n",
    "\n",
    "    elif name in [\"5b\", \"5c\"]:\n",
    "        raw_df = prepare_dataframe5(name)\n",
    "        \n",
    "        splits = split5b(raw_df)\n",
    "        \n",
    "        for claims_df, liabilities_df in zip(splits[::2], splits[1::2]):\n",
    "            claims_df = prepare_all(name, claims_df)\n",
    "            claims_outliers = detect_outliers_iqr(name, claims_df)\n",
    "            all_outliers.append(claims_outliers)\n",
    "\n",
    "            liabilities_df = prepare_all(name, liabilities_df)\n",
    "            liabilities_outliers = detect_outliers_iqr(name, liabilities_df)\n",
    "            all_outliers.append(liabilities_outliers)\n",
    "\n",
    "    elif name == \"5d\":\n",
    "        raw_df = prepare_dataframe5(name)\n",
    "        \n",
    "        splits = split5d(raw_df)\n",
    "        \n",
    "        for claims_df, liabilities_df in zip(splits[::2], splits[1::2]):\n",
    "            claims_df = prepare_all(name, claims_df)\n",
    "            claims_outliers = detect_outliers_iqr(name, claims_df)\n",
    "            all_outliers.append(claims_outliers)\n",
    "\n",
    "            liabilities_df = prepare_all(name, liabilities_df)\n",
    "            liabilities_outliers = detect_outliers_iqr(name, liabilities_df)\n",
    "            all_outliers.append(liabilities_outliers)\n",
    "\n",
    "    elif name == \"5d.1\":\n",
    "        raw_df = prepare_dataframe5(name)\n",
    "        \n",
    "        claims_df_total, liabilities_df_total, claims_df_r, liabilities_df_r = split5d1(raw_df)\n",
    "        \n",
    "        for df in [claims_df_total, liabilities_df_total, claims_df_r, liabilities_df_r]:\n",
    "            df = prepare_all(name, df)\n",
    "            outliers = detect_outliers_iqr(name, df)\n",
    "            all_outliers.append(outliers)\n",
    "\n",
    "    elif name == \"5.d.2\":\n",
    "        raw_df = prepare_dataframe5(name)\n",
    "        \n",
    "        claims_df_total, liabilities_df_total, claims_df_nb, liabilities_df_nb, claims_df_nf, liabilities_df_nf = split5d2(raw_df)\n",
    "        \n",
    "        for df in [claims_df_total, liabilities_df_total, claims_df_nb, liabilities_df_nb, claims_df_nf, liabilities_df_nf]:\n",
    "            df = prepare_all(name, df)\n",
    "            outliers = detect_outliers_iqr(name, df)\n",
    "            all_outliers.append(outliers)\n",
    "\n",
    "    else:\n",
    "        raw_df = prepare_dataframe(name, 2)\n",
    "        checked_df = prepare_all(name, raw_df)\n",
    "        checked_df = checked_df.iloc[1:]\n",
    "        outliers_df = detect_outliers_iqr(name, checked_df)\n",
    "        all_outliers.append(outliers_df)\n",
    "\n",
    "# Concatenate all outliers and export to a single CSV file\n",
    "combined_outliers = pd.concat(all_outliers, ignore_index=True)\n",
    "combined_outliers.insert(0, 'No.', range(1, len(combined_outliers) + 1))\n",
    "\n",
    "combined_outliers.to_csv('/Users/ferroyudisthira/Desktop/DSTA_DQAD/V&H_Check/data/data_pencilan_sski.csv', index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
